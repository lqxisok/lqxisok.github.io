<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Underwater SLAM & Stereo Matching Platform - Qingxuan Lv</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Courier New', 'Consolas', 'Monaco', monospace;
            line-height: 1.6;
            color: #e0f4ff;
            background: linear-gradient(135deg, #001a33 0%, #003366 50%, #004d73 100%);
            min-height: 100vh;
            position: relative;
            overflow-x: hidden;
        }

        /* Animated underwater effect */
        body::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background:
                radial-gradient(ellipse at 20% 30%, rgba(0, 102, 204, 0.1) 0%, transparent 50%),
                radial-gradient(ellipse at 80% 70%, rgba(0, 153, 204, 0.1) 0%, transparent 50%);
            pointer-events: none;
            z-index: 0;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 60px 30px;
            position: relative;
            z-index: 1;
        }

        /* Navigation */
        .nav {
            margin-bottom: 40px;
        }

        .nav a {
            color: #5dade2;
            text-decoration: none;
            font-size: 0.9rem;
            border: 1px solid #5dade2;
            padding: 6px 12px;
            transition: all 0.3s;
            display: inline-block;
        }

        .nav a:hover {
            background-color: #5dade2;
            color: #001a33;
        }

        .nav a::before {
            content: '← ';
        }

        /* Header Section */
        header {
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 2px solid #0d7fb8;
            background: rgba(0, 51, 102, 0.3);
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 4px 20px rgba(0, 153, 204, 0.2);
        }

        header h1 {
            font-size: 2rem;
            font-weight: 700;
            margin-bottom: 15px;
            color: #5dade2;
            letter-spacing: -0.5px;
            text-shadow: 0 0 10px rgba(93, 173, 226, 0.3);
        }

        header .subtitle {
            font-size: 1.1rem;
            color: #85c1e9;
            margin-bottom: 20px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
        }

        .tech-badge {
            display: inline-block;
            background: rgba(93, 173, 226, 0.2);
            color: #5dade2;
            padding: 5px 12px;
            margin-right: 10px;
            margin-top: 10px;
            border: 1px solid #5dade2;
            font-size: 0.85rem;
            border-radius: 4px;
        }

        /* Section Styles */
        section {
            margin-bottom: 50px;
            background: rgba(0, 51, 102, 0.2);
            padding: 30px;
            border-radius: 8px;
            border-left: 4px solid #0d7fb8;
            backdrop-filter: blur(10px);
        }

        h2 {
            font-size: 1.3rem;
            font-weight: 700;
            margin-bottom: 20px;
            color: #5dade2;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        h2::before {
            content: '# ';
            opacity: 0.7;
            color: #0d7fb8;
        }

        h3 {
            font-size: 1.1rem;
            font-weight: 600;
            margin-top: 25px;
            margin-bottom: 15px;
            color: #85c1e9;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
        }

        h3::before {
            content: '> ';
            opacity: 0.7;
        }

        p {
            color: #d6eaf8;
            margin-bottom: 15px;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            font-size: 0.95rem;
        }

        /* Feature Grid */
        .feature-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-top: 25px;
        }

        .feature-card {
            background: rgba(13, 127, 184, 0.15);
            padding: 20px;
            border-radius: 6px;
            border: 1px solid #0d7fb8;
            transition: all 0.3s;
        }

        .feature-card:hover {
            background: rgba(13, 127, 184, 0.25);
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(93, 173, 226, 0.2);
        }

        .feature-card h4 {
            color: #5dade2;
            font-size: 1rem;
            margin-bottom: 10px;
            font-weight: 600;
        }

        .feature-card p {
            font-size: 0.85rem;
            color: #aed6f1;
        }

        /* API Example */
        .api-block {
            background: rgba(0, 26, 51, 0.6);
            border: 1px solid #0d7fb8;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }

        .api-block pre {
            color: #85c1e9;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.6;
        }

        .api-block code {
            color: #5dade2;
        }

        .endpoint {
            color: #52be80;
            font-weight: 600;
        }

        /* Research Highlights */
        .highlight-box {
            background: linear-gradient(135deg, rgba(13, 127, 184, 0.2) 0%, rgba(93, 173, 226, 0.1) 100%);
            border: 1px solid #5dade2;
            border-radius: 6px;
            padding: 20px;
            margin: 20px 0;
        }

        .highlight-box strong {
            color: #5dade2;
        }

        /* List Styles */
        ul {
            list-style-type: none;
            padding-left: 0;
        }

        li {
            margin-bottom: 12px;
            padding-left: 25px;
            position: relative;
            color: #d6eaf8;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            font-size: 0.9rem;
        }

        li::before {
            content: '▸';
            position: absolute;
            left: 0;
            color: #0d7fb8;
            font-weight: 700;
        }

        /* Contact Section */
        .contact-section {
            background: rgba(13, 127, 184, 0.2);
            border: 2px solid #5dade2;
            border-radius: 8px;
            padding: 30px;
            text-align: center;
        }

        .contact-section h2 {
            margin-bottom: 20px;
        }

        .contact-section p {
            font-size: 1rem;
            margin-bottom: 20px;
        }

        .email-link {
            display: inline-block;
            color: #5dade2;
            text-decoration: none;
            font-size: 1.1rem;
            padding: 12px 24px;
            border: 2px solid #5dade2;
            border-radius: 6px;
            transition: all 0.3s;
            font-weight: 600;
        }

        .email-link:hover {
            background-color: #5dade2;
            color: #001a33;
            box-shadow: 0 0 20px rgba(93, 173, 226, 0.4);
        }

        .email-link::before {
            content: '✉ ';
        }

        /* Footer */
        footer {
            margin-top: 60px;
            padding-top: 20px;
            border-top: 1px solid #0d7fb8;
            text-align: center;
            font-size: 0.75rem;
            color: #7fb3d5;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 40px 20px;
            }

            header h1 {
                font-size: 1.5rem;
            }

            .feature-grid {
                grid-template-columns: 1fr;
            }
        }

        /* Subtle animation */
        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-10px); }
        }

        .feature-card {
            animation: float 6s ease-in-out infinite;
        }

        .feature-card:nth-child(2) {
            animation-delay: 1s;
        }

        .feature-card:nth-child(3) {
            animation-delay: 2s;
        }

        /* Video Styles */
        .video-container {
            margin: 25px 0;
            background: rgba(0, 26, 51, 0.4);
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #0d7fb8;
        }

        .video-container h4 {
            color: #5dade2;
            font-size: 1.05rem;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .video-container p {
            font-size: 0.9rem;
            color: #aed6f1;
            margin-bottom: 15px;
        }

        .video-wrapper {
            position: relative;
            width: 100%;
            border-radius: 6px;
            overflow: hidden;
            border: 2px solid #0d7fb8;
            background: #000;
        }

        .video-wrapper video {
            width: 100%;
            height: auto;
            display: block;
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="index.html">Back to Main Page</a>
        </nav>

        <header>
            <h1>Underwater SLAM & Stereo Matching Platform</h1>
            <p class="subtitle">HTTP-Based Backend Server for Underwater Computer Vision</p>
            <div>
                <span class="tech-badge">ORB-SLAM3</span>
                <span class="tech-badge">Underwater Stereo Matching</span>
                <span class="tech-badge">RESTful API</span>
                <span class="tech-badge">Real-time Processing</span>
            </div>
        </header>

        <section id="overview">
            <h2>Overview</h2>
            <p>
                This platform provides a unified HTTP-based backend server that enables researchers and developers to access state-of-the-art underwater computer vision algorithms through simple API calls. The system integrates ORB-SLAM3 for simultaneous localization and mapping with advanced underwater stereo matching techniques, specifically designed to handle the unique challenges of underwater environments.
            </p>
            <div class="highlight-box">
                <p><strong>Key Innovation:</strong> By combining SLAM and stereo matching in a unified framework, this platform enables comprehensive 3D reconstruction and navigation in challenging underwater conditions where traditional methods struggle with light attenuation, color distortion, and low visibility.</p>
            </div>
        </section>

        <section id="features">
            <h2>Core Features</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>ORB-SLAM3 Integration</h4>
                    <p>Real-time camera tracking and mapping with support for monocular, stereo, and RGB-D configurations optimized for underwater scenarios.</p>
                </div>
                <div class="feature-card">
                    <h4>Underwater Stereo Matching</h4>
                    <p>Advanced disparity estimation using deep learning models trained on underwater-specific datasets (UWStereo) to handle scattering and absorption effects.</p>
                </div>
                <div class="feature-card">
                    <h4>HTTP RESTful API</h4>
                    <p>Simple, well-documented API endpoints that allow seamless integration with existing systems and workflows without complex dependencies.</p>
                </div>
                <div class="feature-card">
                    <h4>Batch Processing</h4>
                    <p>Support for processing multiple image sequences with asynchronous job management and result retrieval.</p>
                </div>
                <!-- <div class="feature-card">
                    <h4>Real-time Visualization</h4>
                    <p>Optional WebSocket-based streaming for live trajectory and depth map visualization during processing.</p>
                </div> -->
                <div class="feature-card">
                    <h4>Configurable Parameters</h4>
                    <p>Fine-tune algorithm parameters including feature detection thresholds, matching criteria, and SLAM initialization settings.</p>
                </div>
            </div>
        </section>

        <section id="architecture">
            <h2>System Architecture</h2>

            <h3>Backend Components</h3>
            <ul>
                <li><strong>API Server:</strong> FastAPI-based REST interface handling client requests and job management</li>
                <li><strong>SLAM Engine:</strong> ORB-SLAM3 core with underwater-specific calibration</li>
                <li><strong>Stereo Matching Module:</strong> Iterative deep neural network for disparity estimation with underwater color correction preprocessing</li>
                <li><strong>Result Storage:</strong> Efficient supabase and local storage system for trajectories, point clouds, and depth maps</li>
            </ul>

            <h3>API Endpoints</h3>
            <div class="api-block">
                <pre><code><span class="endpoint">Project</span> /api/v1/project/*
Project management (create, list, delete projects)
                    
<span class="endpoint">SLAM</span> /api/v1/slam/*
Submit stereo image sequence for SLAM processing

<span class="endpoint">Stereo Matching</span> /api/v1/stereo_matching/*
Compute disparity map for stereo image pair

<span class="endpoint">PointCloud</span> /api/v1/job/pointcloud/*
Check processing status

<span class="endpoint">Reconstruction</span> /api/v1/reconstruction/*
Retrieve processing results (trajectory, point cloud, depth maps)</code></pre>
            </div>
        </section>

        <section id="research">
            <h2>Research Foundation</h2>
            <p>
                This platform builds upon cutting-edge research in underwater computer vision, particularly focusing on:
            </p>

            <h3>Underwater Stereo Matching</h3>
            <p>
                Traditional stereo matching algorithms fail in underwater environments due to unique optical phenomena. This platform incorporates novel techniques to address:
            </p>
            <ul>
                <li>Wavelength-dependent light attenuation causing color distortion</li>
                <li>Backscattering from suspended particles reducing contrast</li>
                <li>Refraction effects from camera housings</li>
                <li>Non-uniform illumination in natural underwater scenes</li>
            </ul>

            <div class="highlight-box">
                <p>
                    <strong>UWStereo Dataset:</strong> The stereo matching models are trained on the UWStereo dataset, a large-scale synthetic dataset specifically designed for underwater scenarios with accurate ground truth disparity maps.
                </p>
            </div>

            <h3>Underwater SLAM Challenges</h3>
            <p>
                ORB-SLAM3 is adapted for underwater use with modifications including:
            </p>
            <ul>
                <li>Robust feature detection under varying turbidity conditions</li>
                <li>Motion model adjustments for underwater vehicle dynamics</li>
                <li>Loop closure detection with appearance variations due to lighting changes</li>
                <li>Integration of depth information from stereo matching to improve map density</li>
            </ul>
        </section>

        <section id="applications">
            <h2>Applications</h2>
            <div class="feature-grid">
                <div class="feature-card">
                    <h4>Autonomous Underwater Vehicles (AUVs)</h4>
                    <p>Real-time navigation and obstacle avoidance for underwater robots in complex environments.</p>
                </div>
                <div class="feature-card">
                    <h4>Marine Biology Research</h4>
                    <p>3D reconstruction of coral reefs and underwater habitats for monitoring and conservation.</p>
                </div>
                <div class="feature-card">
                    <h4>Underwater Archaeology</h4>
                    <p>Precise mapping and documentation of shipwrecks and submerged archaeological sites.</p>
                </div>
                <div class="feature-card">
                    <h4>Infrastructure Inspection</h4>
                    <p>Automated inspection of underwater structures including pipelines, cables, and offshore platforms.</p>
                </div>
            </div>


            <h2>Examples</h2>

            <div class="video-container">
                <h4>Underwater Data Synthesis</h4>
                <p>This video demonstrates the data synthesizing process for generating realistic underwater stereo image pairs with ground truth disparity maps. The synthesis pipeline simulates underwater optical effects including light attenuation, scattering, and color distortion to create training data for deep learning models.</p>
                <div class="video-wrapper">
                    <video controls>
                        <source src="asserts/underwater_datasynth.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>

            <div class="video-container">
                <h4>Underwater Stereo Matching Results</h4>
                <p>This video showcases the stereo matching results on real underwater scenes. The algorithm effectively handles challenging conditions such as low contrast, color cast, and floating particles to produce accurate depth estimations. The visualization compares the input stereo pairs with the computed disparity maps.</p>
                <div class="video-wrapper">
                    <video controls>
                        <source src="asserts/underwater_stereomatching.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>
        </section>

        <section id="getting-started">
            <h2>Getting Started</h2>

            <h3>Requirements</h3>
            <ul>
                <li>Calibrated stereo camera system (intrinsic and extrinsic parameters)</li>
                <li>Image sequences in standard formats (PNG, JPEG) or video streams</li>
                <li>Network access to the backend server</li>
                <li>API authentication credentials (provided upon registration)</li>
            </ul>

            <h3>Deployment Options</h3>
            <ul>
                <li><strong>Self-Hosted:</strong> Deploy on your own server with sufficient hardware requirements.</li>
                <li><strong>Edge Computing:</strong> Lightweight version for deployment on underwater vehicles.</li>
            </ul>

            <h3>Documentation</h3>
            <p>
                Comprehensive API documentation, tutorials, and example code are available in python programming languages including.
            </p>
        </section>

        <section id="contact" class="contact-section">
            <h2>Contact & Collaboration</h2>
            <p>
                Interested in using this platform for your research or project? Want to collaborate on extending the capabilities?
            </p>
            <p>
                Feel free to reach out to discuss technical details, request access, or explore collaboration opportunities.
            </p>
            <a href="mailto:lvqingxuan@stu.ouc.edu.cn" class="email-link">lvqingxuan@stu.ouc.edu.cn</a>
        </section>

        <footer>
            <p>Underwater SLAM & Stereo Matching Platform | Developed by Qingxuan Lv | Ocean University of China</p>
            <p>Last updated: January 2025</p>
        </footer>
    </div>
</body>
</html>
